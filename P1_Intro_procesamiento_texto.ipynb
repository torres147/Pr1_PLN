{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5UQq8NKVhFF"
      },
      "source": [
        "# PrÃ¡ctica 1. Preprocesamiento y TokenizaciÃ³n de Texto\n",
        "\n",
        "> **Nota:** Este notebook asume Python 3.8+ con las bibliotecas `nltk`, `transformers`, `emoji` y `tokenizers` instaladas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMRNpQb6VhFI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# InstalaciÃ³n de dependencias (ejecutar solo la primera vez)\n",
        "!pip install -q nltk transformers emoji tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHTr3ryGVhFJ"
      },
      "source": [
        "## 1. IntroducciÃ³n\n",
        "\n",
        "El **preprocesamiento** es la primera etapa en cualquier *pipeline* de Procesamiento del Lenguaje Natural (PLN). Su objetivo es limpiar y estructurar el texto de entrada para facilitar la extracciÃ³n de informaciÃ³n Ãºtil.\n",
        "\n",
        "### Â¿Por quÃ© es necesario?\n",
        "\n",
        "Los modelos de PLN trabajan con representaciones numÃ©ricas del lenguaje. Por ejemplo, en un texto publicado en X, un texto sin limpiar o preprocesar suele presentar:\n",
        "\n",
        "| Problema | Ejemplo | Consecuencia |\n",
        "|---|---|---|\n",
        "| URLs Ãºnicas | `https://t.co/abc123` | Inflan el vocabulario sin aportar significado |\n",
        "| Menciones | `@usuario1`, `@usuario2` | Cada usuario es un token distinto |\n",
        "| MayÃºsculas inconsistentes | `Casa` vs `casa` | Se tratan como palabras diferentes. |\n",
        "| Caracteres elongados | `buenoooo` | Dificultan la normalizaciÃ³n. Cada elongaciÃ³n con longitud diferente se tratan como palabras diferentes |\n",
        "| Emojis | ğŸ˜, ğŸ”¥ | Son informaciÃ³n valiosa (emociÃ³n/valoraciÃ³n/evaluaciÃ³n) o ruido segÃºn la tarea |\n",
        "\n",
        "En esta prÃ¡ctica trabajaremos con tuits en espaÃ±ol â€” un corpus especialmente desafiante por su lenguaje informal, abreviaciones y mezcla de idiomas (*code-switching*).\n",
        "\n",
        "### Pipeline tÃ­pico de preprocesamiento\n",
        "\n",
        "```\n",
        "Texto crudo â†’ NormalizaciÃ³n â†’ TokenizaciÃ³n â†’ Filtrado â†’ RepresentaciÃ³n vectorial\n",
        "```\n",
        "\n",
        "Cada paso reduce la dispersiÃ³n del vocabulario y mejora la calidad de las representaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ndfpqbKVhFJ"
      },
      "source": [
        "## 2. NormalizaciÃ³n de Texto\n",
        "\n",
        "La **normalizaciÃ³n** convierte el texto a una forma canÃ³nica. No existe una receta universal: las transformaciones dependen de la tarea (*clasificaciÃ³n de sentimientos* â‰  *reconocimiento de entidades*).\n",
        "\n",
        "### Principio general\n",
        "> *Cuanta mÃ¡s informaciÃ³n eliminamos, mÃ¡s fÃ¡cil resulta el aprendizaje automÃ¡tico pero mayor es el riesgo de perder seÃ±al relevante.*\n",
        "\n",
        "Las transformaciones mÃ¡s habituales son:\n",
        "- ConversiÃ³n a minÃºsculas\n",
        "- EliminaciÃ³n/sustituciÃ³n de URLs y menciones\n",
        "- Tratamiento de emojis\n",
        "- ReducciÃ³n de elongaciones (\"buenoooo\" â†’ \"bueno\")\n",
        "- EliminaciÃ³n de signos de puntuaciÃ³n redundantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWAOTF-EVhFK"
      },
      "source": [
        "### 2.1. Expresiones Regulares para Usuarios y URLs\n",
        "\n",
        "Las **expresiones regulares** (regex) son patrones que permiten identificar y transformar cadenas de texto de manera eficiente.\n",
        "\n",
        "#### AnatomÃ­a de una regex Ãºtil en Twitter\n",
        "\n",
        "| PatrÃ³n | Captura | Ejemplo |\n",
        "|---|---|---|\n",
        "| `@\\w+` | Menciones | `@usuario`, `@NLP_Lab` |\n",
        "| `https?://\\S+` | URLs con protocolo | `https://t.co/abc` |\n",
        "| `www\\.\\S+` | URLs sin protocolo | `www.ejemplo.com` |\n",
        "| `#\\w+` | Hashtags | `#MachineLearning` |\n",
        "| `(.)\\1{2,}` | Caracteres repetidos | `buenoooo` |\n",
        "\n",
        "#### Â¿Por quÃ© normalizar con tokens especiales?\n",
        "\n",
        "En lugar de eliminar `@usuario` directamente, se sustituye por `[USER]`. Esto:\n",
        "1. **Preserva informaciÃ³n estructural**: el modelo sabe que *alguien* fue mencionado.\n",
        "2. **Reduce el vocabulario**: 10.000 usuarios distintos â†’ 1 token.\n",
        "3. **Facilita la anonimizaciÃ³n** del corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00S7fGYBVhFK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# â”€â”€ FunciÃ³n de normalizaciÃ³n bÃ¡sica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def normalizar_basico(texto):\n",
        "    \"\"\"Sustituye URLs y menciones por tokens especiales.\"\"\"\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', texto)  # URLs\n",
        "    texto = re.sub(r'@\\w+', '[USER]', texto)                    # Menciones\n",
        "    return texto\n",
        "\n",
        "def reducir_elongacion(texto):\n",
        "    \"\"\"'buenoooo' â†’ 'buenoo' (conserva 2 para indicar Ã©nfasis).\"\"\"\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', texto)\n",
        "\n",
        "def normalizar_hashtag(texto):\n",
        "    \"\"\"Extrae el texto de los hashtags eliminando el sÃ­mbolo #.\"\"\"\n",
        "    return re.sub(r'#(\\w+)', r'\\1', texto)\n",
        "\n",
        "# â”€â”€ BaterÃ­a de ejemplos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ejemplos = [\n",
        "    \"IncreÃ­ble charla de PLN en @universidad! MÃ¡s info en https://t.co/nlp123\",\n",
        "    \"QuÃ© buenoooo el partido de @RealMadridCF ğŸ”¥ #LaLiga www.marca.com\",\n",
        "    \"Hola @pepito y @maria, revisad http://docs.python.org pleaaaseee!!!\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 65)\n",
        "for ej in ejemplos:\n",
        "    norm = normalizar_basico(ej)\n",
        "    elon = reducir_elongacion(norm)\n",
        "    print(f\"Original  : {ej}\")\n",
        "    print(f\"Norm.     : {norm}\")\n",
        "    print(f\"Sin elong.: {elon}\")\n",
        "    print(\"-\" * 65)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcHMrhxPVhFK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# â”€â”€ Comparativa cuantitativa â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "corpus_crudo = [\n",
        "    \"Visita https://openai.com y sigue a @OpenAI para novedades!\",\n",
        "    \"El modelo de @Google estÃ¡ en https://cloud.google.com/ai\",\n",
        "    \"Me ha encantado la presentaciÃ³n de @usuario1 en https://t.co/xyz\",\n",
        "    \"IncreÃ­ble!! @maria y @juan hablan de PLN en www.congreso-ia.es\",\n",
        "]\n",
        "\n",
        "corpus_norm = [normalizar_basico(t) for t in corpus_crudo]\n",
        "\n",
        "vocab_crudo = set(w for t in corpus_crudo for w in t.split())\n",
        "vocab_norm  = set(w for t in corpus_norm  for w in t.split())\n",
        "\n",
        "print(f\"Tokens Ãºnicos ANTES de normalizar : {len(vocab_crudo)}\")\n",
        "print(f\"Tokens Ãºnicos DESPUÃ‰S de normalizar: {len(vocab_norm)}\")\n",
        "print(f\"ReducciÃ³n: {len(vocab_crudo) - len(vocab_norm)} tokens ({((len(vocab_crudo)-len(vocab_norm))/len(vocab_crudo))*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J84ZNgLDVhFL"
      },
      "source": [
        "### 2.2. Manejo de Emojis\n",
        "\n",
        "Los emojis son **caracteres Unicode** que transmiten emociÃ³n, ironÃ­a y contexto con gran eficiencia. En Twitter, mÃ¡s del 25 % de los mensajes contienen al menos un emoji.\n",
        "\n",
        "#### Estrategias de tratamiento\n",
        "\n",
        "| Estrategia | CuÃ¡ndo usarla | Resultado para ğŸ˜ |\n",
        "|---|---|---|\n",
        "| **Eliminar** | Tarea donde el sentimiento no importa (p. ej., extracciÃ³n de entidades) | `\"\"` |\n",
        "| **Conservar** | Modelos modernos (BERT, GPT) que incluyen emojis en su vocabulario | `\"ğŸ˜\"` |\n",
        "| **Traducir a texto** | Modelos basados en palabras, anÃ¡lisis de sentimiento | `\"cara sonriente con ojos de corazÃ³n\"` |\n",
        "\n",
        "#### La biblioteca `emoji`\n",
        "Permite convertir emojis a su descripciÃ³n textual oficial (en mÃºltiples idiomas) o eliminarlos sistemÃ¡ticamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b-4CHQAVhFL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import emoji\n",
        "import re\n",
        "\n",
        "# â”€â”€ 1. Eliminar emojis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def eliminar_emojis(texto):\n",
        "    return emoji.replace_emoji(texto, replace='')\n",
        "\n",
        "# â”€â”€ 2. Traducir emojis a texto â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def emojis_a_texto(texto, idioma='es'):\n",
        "    return emoji.demojize(texto, language=idioma, delimiters=('[', ']'))\n",
        "\n",
        "# â”€â”€ 3. Extraer lista de emojis presentes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def extraer_emojis(texto):\n",
        "    return [c for c in texto if c in emoji.EMOJI_DATA]\n",
        "\n",
        "# â”€â”€ Ejemplos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tweets = [\n",
        "    \"Me encanta el PLN! ğŸ˜ Es lo mejor ğŸ”¥ğŸ”¥ğŸ”¥\",\n",
        "    \"Â¡Victoria! ğŸ†ğŸ‰ @Seleccion gana el mundial\",\n",
        "    \"Este bug me tiene ğŸ˜¤ğŸ˜¤ ya no puedo mÃ¡s ğŸ’€\",\n",
        "]\n",
        "\n",
        "print(f\"{'Original':<45} | {'Sin emojis':<35} | {'Emojis encontrados'}\")\n",
        "print(\"-\" * 110)\n",
        "for t in tweets:\n",
        "    sin = eliminar_emojis(t).strip()\n",
        "    found = extraer_emojis(t)\n",
        "    print(f\"{t:<45} | {sin:<35} | {found}\")\n",
        "\n",
        "print()\n",
        "print(\"â”€â”€ TraducciÃ³n a texto â”€â”€\")\n",
        "for t in tweets:\n",
        "    print(f\"  {t}\")\n",
        "    print(f\"  â†’ {emojis_a_texto(t)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyoIJNHsVhFL"
      },
      "source": [
        "## 3. TokenizaciÃ³n: De la Palabra a la Subpalabra\n",
        "\n",
        "La **tokenizaciÃ³n** es el proceso de segmentar una cadena de texto en unidades mÃ­nimas de significado llamadas **tokens**. Es la piedra angular de cualquier sistema PLN: todo modelo recibe una secuencia de tokens como entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFhgumYfVhFM"
      },
      "source": [
        "### 3.1. TokenizaciÃ³n por Espacios (`str.split`)\n",
        "\n",
        "El mÃ©todo mÃ¡s primitivo: divide el texto en cada espacio en blanco.\n",
        "\n",
        "**Ventajas:** RÃ¡pido, sin dependencias, determinista.\n",
        "\n",
        "**Problemas:**\n",
        "- `\"hola!\"` â‰  `\"hola\"` â†’ vocabulario inflado innecesariamente.\n",
        "- No trata correctamente contracciones (`\"don't\"` â†’ `[\"don't\"]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLdsPf1IVhFM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# â”€â”€ TokenizaciÃ³n por split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ejemplos_split = [\n",
        "    \"El gato come. Â¿El perro tambiÃ©n come?\",\n",
        "    \"Me encanta el PLN! ğŸ˜ #MachineLearning\",\n",
        "    \"URL: https://t.co/abc y menciÃ³n: @usuario\",\n",
        "]\n",
        "\n",
        "print(\"â”€â”€ TokenizaciÃ³n con str.split() â”€â”€\")\n",
        "for texto in ejemplos_split:\n",
        "    tokens = texto.split()\n",
        "    print(f\"  Texto  : {texto}\")\n",
        "    print(f\"  Tokens : {tokens}\")\n",
        "    print(f\"  NÂº tok.: {len(tokens)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if2xaJAeVhFM"
      },
      "source": [
        "### 3.2. TokenizaciÃ³n LingÃ¼Ã­stica: NLTK TweetTokenizer\n",
        "\n",
        "NLTK es una librerÃ­a de Python para PLN con una gran cantidad de mÃ©todos que facilitan la programaciÃ³n de sistemas de PLN. Entre los disntitos mÃ©todos que ofrece estÃ¡n distintos tipos de *tokenizadores*. [Paquete de tokenizaciÃ³n](https://www.nltk.org/api/nltk.tokenize.html), [Ejemplos](https://www.nltk.org/howto/tokenize.html).\n",
        "\n",
        "NLTK ofrece un tokenizador **especÃ­fico para redes sociales**, en particular para X, que aplica reglas lingÃ¼Ã­sticas para manejar correctamente:\n",
        "- Signos de puntuaciÃ³n: los separa como tokens independientes.\n",
        "- Hashtags: los conserva como unidad (`#MachineLearning` â†’ 1 token).\n",
        "- Emojis: los trata como tokens propios.\n",
        "- Contracciones en inglÃ©s: `\"don't\"` â†’ `[\"do\", \"n't\"]`.\n",
        "\n",
        "#### ParÃ¡metros clave de `TweetTokenizer`\n",
        "\n",
        "| ParÃ¡metro | Por defecto | Efecto |\n",
        "|---|---|---|\n",
        "| `preserve_case` | `True` | Si `False`, convierte a minÃºsculas |\n",
        "| `reduce_len` | `False` | Si `True`, reduce elongaciones (\"buenoooo\" â†’ \"buenoo\") |\n",
        "| `strip_handles` | `False` | Si `True`, elimina @menciones |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEGS-D86VhFM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "\n",
        "tweet = \"Â¡Hoooolaaaa! Â¿QuÃ© tal estÃ¡s, seÃ±or GarcÃ­a? Visita www.ejemplo.com o @escribeme ğŸ‰\"\n",
        "\n",
        "tknzr_default  = TweetTokenizer()\n",
        "tknzr_lower    = TweetTokenizer(preserve_case=False)\n",
        "tknzr_reduce   = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "tknzr_nohandle = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "\n",
        "configs = [\n",
        "    (\"Default\",               tknzr_default),\n",
        "    (\"lowercase\",             tknzr_lower),\n",
        "    (\"lowercase + reduce_len\",tknzr_reduce),\n",
        "    (\"+ strip_handles\",       tknzr_nohandle),\n",
        "]\n",
        "\n",
        "print(f\"Tweet original: {tweet}\\n\")\n",
        "for nombre, tkn in configs:\n",
        "    print(f\"  [{nombre}]\")\n",
        "    print(f\"    {tkn.tokenize(tweet)}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"â”€â”€ Comparativa split vs TweetTokenizer â”€â”€\")\n",
        "print(f\"split          : {tweet.split()}\")\n",
        "print(f\"TweetTokenizer : {TweetTokenizer().tokenize(tweet)}\")\n",
        "print(f\"word_tokenize  : {word_tokenize(tweet, language='spanish')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsWGvGhIVhFM"
      },
      "source": [
        "### 3.3. TokenizaciÃ³n por Subpalabras\n",
        "\n",
        "Los tokenizadores de **subpalabras** son el estÃ¡ndar de la industria desde 2018. En lugar de dividir por palabras completas o por caracteres individuales, aprenden un vocabulario de *fragmentos frecuentes* a partir de un corpus grande. El resultado es un equilibrio Ã³ptimo entre:\n",
        "\n",
        "- **Cobertura:** toda palabra desconocida puede descomponerse en fragmentos conocidos â†’ **cero `<UNK>`**.\n",
        "- **Eficiencia:** el vocabulario es finito y manejable (10.000â€“256.000 entradas).\n",
        "- **MorfologÃ­a:** se capturan prefijos y sufijos recurrentes (`pre-`, `-ciÃ³n`, `-mente`).\n",
        "\n",
        "El mÃ¡s conocido es **BPE**, pero existen otros como **WordPiece**. Un poco mÃ¡s de *tokenizadores* a nivel de subpalabra en [HuggingFace](https://huggingface.co/docs/transformers/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3.1. Byte Pair Encoding (BPE)\n",
        "\n",
        "**Idea central:** Construir el vocabulario fusionando iterativamente el par de sÃ­mbolos mÃ¡s frecuente del corpus.\n",
        "\n",
        "##### Algoritmo paso a paso\n",
        "\n",
        "```\n",
        "Corpus de entrenamiento (simplificado, 4 palabras):\n",
        "  \"bajo\" Ã— 5,  \"baja\" Ã— 3,  \"trabajo\" Ã— 4,  \"trabaja\" Ã— 2\n",
        "\n",
        "â”€â”€â”€ InicializaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Cada palabra se descompone en caracteres + marcador de fin </w>:\n",
        "  b a j o         Ã— 5\n",
        "  b a j a         Ã— 3\n",
        "  t r a b a j o   Ã— 4\n",
        "  t r a b a j a   Ã— 2\n",
        "\n",
        "â”€â”€â”€ IteraciÃ³n 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Par mÃ¡s frecuente: \"a j\"  (5+3+4+2 = 14 veces)\n",
        "â†’ Fusionar:  b [aj] o,  b [aj] a,  t r a b [aj] o ...\n",
        "\n",
        "â”€â”€â”€ IteraciÃ³n 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Par mÃ¡s frecuente: \"aj o\"  (5+4 = 9 veces)\n",
        "â†’ Fusionar: b [ajo],  t r a b [ajo] ...\n",
        "\n",
        "â”€â”€â”€ Tras N iteraciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Seguimos iterando hasta llegar al tamaÃ±o de vocabulario deseado.\n",
        "```\n",
        "\n",
        "##### Byte-level BPE: la variante moderna\n",
        "\n",
        "Los modelos mÃ¡s recientes operan sobre **bytes UTF-8** en lugar de caracteres Unicode. Esto garantiza que *cualquier texto en cualquier idioma* puede tokenizarse sin caracteres desconocidos, ya que todo texto se puede representar con solo 256 sÃ­mbolos base (un byte por sÃ­mbolo).\n",
        "\n",
        "| Modelo | Vocab | AÃ±o | Notas |\n",
        "|---|---|---|---|\n",
        "| **GPT-4o** | ~200 k | 2024 | `o200k_base`, optimizado para cÃ³digo y multilingÃ¼e |\n",
        "| **LLaMA 3.1 / 3.2** | 128 k | 2024 | Gran mejora sobre LLaMA 2 (32 k), mejor cobertura multilingÃ¼e |\n",
        "| **Mistral / Mixtral** | 32 k | 2023 | Compacto y eficiente para despliegue local |\n",
        "| **Phi-4** | 100 k | 2024 | Heredado de tiktoken, foco en razonamiento |\n",
        "| **DeepSeek-V3** | 128 k | 2024 | Entrenado masivamente en chino + inglÃ©s + cÃ³digo |\n",
        "| **Qwen 2.5** | 152 k | 2024 | Vocabulario ampliado para chino y cÃ³digo |\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3.4. Resumen comparativo\n",
        "\n",
        "| Propiedad | BPE | WordPiece |\n",
        "|---|---|---|\n",
        "| **Criterio de selecciÃ³n** | Frecuencia del par | PMI / verosimilitud |\n",
        "| **Marcador de continuaciÃ³n** | Ninguno (`Ä ` en byte-level) | `##` al inicio del subword |\n",
        "| **Modelos representativos (2024)** | GPT-4o, LLaMA 3, Phi-4 | ModernBERT, DeBERTa-v3 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKrFlmBkVhFN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_vocab(corpus: dict) -> dict:\n",
        "    \"\"\"Representa cada palabra como secuencia de caracteres + </w>.\"\"\"\n",
        "    return {' '.join(list(word)) + ' </w>': freq for word, freq in corpus.items()}\n",
        "\n",
        "def get_pairs(vocab: dict) -> Counter:\n",
        "    \"\"\"Cuenta todos los pares de sÃ­mbolos adyacentes.\"\"\"\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(best_pair: tuple, vocab: dict) -> dict:\n",
        "    \"\"\"Fusiona el mejor par en todas las entradas del vocabulario.\"\"\"\n",
        "    bigram = ' '.join(best_pair)\n",
        "    replacement = ''.join(best_pair)\n",
        "    return {word.replace(bigram, replacement): freq for word, freq in vocab.items()}\n",
        "\n",
        "# â”€â”€ Corpus de ejemplo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "corpus = {'bajo': 5, 'baja': 3, 'trabajo': 4, 'trabaja': 2, 'caja': 6, 'caje': 1}\n",
        "\n",
        "vocab = get_vocab(corpus)\n",
        "print(\"Vocabulario inicial (nivel carÃ¡cter):\")\n",
        "for w, f in vocab.items():\n",
        "    print(f\"  {f}Ã— [{w}]\")\n",
        "\n",
        "# â”€â”€ Ejecutar 8 iteraciones de BPE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "N_MERGES = 8\n",
        "merges = []\n",
        "\n",
        "print(f\"\\n{'â”€'*60}\\nEjecutando {N_MERGES} fusiones BPE...\\n{'â”€'*60}\")\n",
        "for i in range(N_MERGES):\n",
        "    pairs = get_pairs(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best_pair, freq = pairs.most_common(1)[0]\n",
        "    vocab = merge_vocab(best_pair, vocab)\n",
        "    merges.append(best_pair)\n",
        "    merged = ''.join(best_pair)\n",
        "    print(f\"  FusiÃ³n {i+1:>2}: {best_pair[0]!r:8} + {best_pair[1]!r:8} â†’ {merged!r:12} (frec. {freq})\")\n",
        "\n",
        "print(f\"\\nVocabulario FINAL:\")\n",
        "for w, f in sorted(vocab.items(), key=lambda x: -x[1]):\n",
        "    tokens = w.replace(' </w>', '').split()\n",
        "    print(f\"  {f}Ã— {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVhWDxEVhFN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# VisualizaciÃ³n: cÃ³mo segmenta cada tokenizador palabras difÃ­ciles\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok_bpe  = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tok_wordpiece = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "palabras = [\n",
        "    \"tokenizaciÃ³n\",\n",
        "    \"chatgptear\",\n",
        "    \"instagrameable\",\n",
        "    \"microprocesador\",\n",
        "    \"anticonstitucional\",\n",
        "    \"COVID-19\",\n",
        "    \"LLM\",\n",
        "    \"fintuneado\",          # \"fine-tuned\" hispanizado\n",
        "    \"embeddings\",\n",
        "    \"multimodal\",\n",
        "]\n",
        "\n",
        "print(f\"  {'Palabra':<30} | {'BERT (WordPiece)':<55} | {'GPT-2 (Byte BPE)':<55}\")\n",
        "print(\"  \" + \"â”€\" * 130)\n",
        "for p in palabras:\n",
        "    t_b = str(tok_wordpiece.tokenize(p))\n",
        "    t_l = str(tok_bpe.tokenize(p))\n",
        "    print(f\"  {p:<30} | {t_b:<55} | {t_l:<55}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: actualmente ls *tokenizador* es un mÃ³dulo muy importante, dado que cada modelo depende de un determinado *tokenizdor*."
      ],
      "metadata": {
        "id": "GPpYfA4kUKos"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUDFHGJzVhFN"
      },
      "source": [
        "## 4. EliminaciÃ³n de Palabras VacÃ­as (Stopwords)\n",
        "\n",
        "Las **stopwords** (palabras vacÃ­as) son tÃ©rminos muy frecuentes que aportan poco valor semÃ¡ntico por sÃ­ solos: artÃ­culos, preposiciones, conjunciones, pronombres comunes...\n",
        "\n",
        "### Â¿CuÃ¡ndo eliminar stopwords?\n",
        "\n",
        "| Tarea | Â¿Eliminar stopwords? | RazÃ³n |\n",
        "|---|---|---|\n",
        "| ClasificaciÃ³n de documentos | âœ… SÃ­ (generalmente) | Reducen ruido y vocabulario |\n",
        "| Modelos de bolsa de palabras (TF-IDF) | âœ… SÃ­ | Los tÃ©rminos frecuentes dominan sin aportar discriminaciÃ³n |\n",
        "| Modelos de lenguaje (BERT, GPT) | âŒ No | Estos modelos aprenden contexto completo |\n",
        "| TraducciÃ³n automÃ¡tica | âŒ No | GramÃ¡tica completa necesaria |\n",
        "| AnÃ¡lisis de sentimientos | âš ï¸ Cuidado | \"no me gusta\" â†’ si se elimina \"no\", invierte el sentimiento |\n",
        "\n",
        "### âš ï¸ PrecauciÃ³n con la negaciÃ³n\n",
        "\n",
        "Eliminar stopwords puede **destruir la semÃ¡ntica negativa**:\n",
        "\n",
        "```\n",
        "\"no me gusta\"  â†’ eliminar 'no' â†’ \"me gusta\"  â† Â¡Sentimiento invertido!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX3WsY20VhFN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stop_es = set(stopwords.words('spanish'))\n",
        "print(f\"Stopwords en espaÃ±ol (NLTK): {len(stop_es)} tÃ©rminos\")\n",
        "print(f\"Muestra: {sorted(list(stop_es))[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Zn0RY0VhFO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tknzr = TweetTokenizer(preserve_case=False)\n",
        "stop_es = set(stopwords.words('spanish'))\n",
        "\n",
        "tweets_ejemplo = [\n",
        "    \"El aprendizaje automÃ¡tico es el futuro de la inteligencia artificial\",\n",
        "    \"no me gusta nada esta pelÃ­cula, es muy aburrida\",\n",
        "    \"EspaÃ±a gana el partido gracias al gol en el Ãºltimo minuto\",\n",
        "]\n",
        "\n",
        "print(\"â”€â”€ Efecto de eliminar stopwords â”€â”€\\n\")\n",
        "for tweet in tweets_ejemplo:\n",
        "    tokens   = tknzr.tokenize(tweet)\n",
        "    sin_stop = [t for t in tokens if t not in stop_es]\n",
        "    print(f\"  Original   : {tweet}\")\n",
        "    print(f\"  Todos      : {tokens}\")\n",
        "    print(f\"  Sin stop.  : {sin_stop}\")\n",
        "    if 'no' in tokens and 'no' not in sin_stop:\n",
        "        print(f\"  âš ï¸  Se eliminÃ³ 'no' â€” posible inversiÃ³n semÃ¡ntica\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qJn_ZaVhFO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# â”€â”€ Stopwords personalizadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "stop_custom = stop_es.copy()\n",
        "stop_custom.update(['rt', 'vÃ­a', 'cc', '[user]', '[url]', 'q', 'x', 'xq'])\n",
        "palabras_a_conservar = {'no', 'ni', 'nunca', 'jamÃ¡s', 'tampoco', 'sin'}\n",
        "stop_custom -= palabras_a_conservar\n",
        "\n",
        "print(f\"Stopwords personalizadas: {len(stop_custom)}\")\n",
        "print(f\"AÃ±adidas: {stop_custom - stop_es}\")\n",
        "print(f\"Preservadas (eliminadas del filtro): {palabras_a_conservar}\")\n",
        "\n",
        "tweet_neg = \"no me gusta nada este producto, nunca lo recomendarÃ­a\"\n",
        "tokens_neg      = tknzr.tokenize(tweet_neg)\n",
        "filtrado_nltk   = [t for t in tokens_neg if t not in stop_es]\n",
        "filtrado_custom = [t for t in tokens_neg if t not in stop_custom]\n",
        "print(f\"\\nTweet: {tweet_neg}\")\n",
        "print(f\"Stop NLTK  : {filtrado_nltk}\")\n",
        "print(f\"Stop custom: {filtrado_custom}  â† preserva 'no' y 'nunca'\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-NcsB5pVhFO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# â”€â”€ AnÃ¡lisis de frecuencias: con vs sin stopwords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "corpus = [\n",
        "    \"El gato come pescado todos los dÃ­as\",\n",
        "    \"El perro come carne y huesos\",\n",
        "    \"El gato y el perro juegan en el jardÃ­n\",\n",
        "    \"Los animales domÃ©sticos son compaÃ±Ã­a para las personas\",\n",
        "    \"El pÃ¡jaro canta en el Ã¡rbol del jardÃ­n\",\n",
        "]\n",
        "\n",
        "todos_tokens = [t for frase in corpus for t in tknzr.tokenize(frase)]\n",
        "sin_stop_tok = [t for t in todos_tokens if t not in stop_es]\n",
        "\n",
        "freq_con = Counter(todos_tokens)\n",
        "freq_sin = Counter(sin_stop_tok)\n",
        "\n",
        "print(\"â”€â”€ Top 10 tokens CON stopwords â”€â”€\")\n",
        "for token, n in freq_con.most_common(10):\n",
        "    print(f\"  {token:<15} {'â–ˆ' * n} ({n})\")\n",
        "\n",
        "print()\n",
        "print(\"â”€â”€ Top 10 tokens SIN stopwords â”€â”€\")\n",
        "for token, n in freq_sin.most_common(10):\n",
        "    print(f\"  {token:<15} {'â–ˆ' * n} ({n})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 *TokenizciÃ³n* en oraciones o segmentaciÃ³n\n",
        "\n",
        "Aunque estÃ¡ prÃ¡ctica se estÃ¡ centrando en textos procedentes de plataformas de microblogging como X, es posible procesar textos/documntos mÃ¡s laragos, para lo cual es interesante segmentar o identificar las oraciones de dicho texto.\n",
        "\n",
        "NLTK nos ofrece la posiblidad de segmentar en oraciones. Veamos un ejemplo"
      ],
      "metadata": {
        "id": "SRzYndfLW-1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "texto = '''En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha\n",
        "mucho tiempo que vivÃ­a un hidalgo de los de lanza en astillero, adarga antigua,\n",
        "rocÃ­n flaco y galgo corredor. Una olla de algo mÃ¡s vaca que carnero, salpicÃ³n\n",
        "las mÃ¡s noches, duelos y quebrantos los sÃ¡bados, lantejas los viernes, algÃºn\n",
        "palomino de aÃ±adidura los domingos, consumÃ­an las tres partes de su hacienda.\n",
        "El resto della concluÃ­an sayo de velarte, calzas de velludo para las fiestas,\n",
        "con sus pantuflos de lo mismo, y los dÃ­as de entresemana se honraba con su\n",
        "vellorÃ­ de lo mÃ¡s fino. TenÃ­a en su casa una ama que pasaba de los cuarenta,\n",
        "y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que asÃ­\n",
        "ensillaba el rocÃ­n como tomaba la podadera. Frisaba la edad de nuestro hidalgo\n",
        "con los cincuenta aÃ±os; era de complexiÃ³n recia, seco de carnes, enjuto de\n",
        "rostro, gran madrugador y amigo de la caza.'''\n",
        "\n",
        "sentences = sent_tokenize(texto)\n",
        "sentences = [s.replace('\\n','') for s in sentences]\n",
        "print(f\"NÃºmero de oraciones: {len(sentences)}\")\n",
        "for i in range(len(sentences)):\n",
        "  print(f\"OraciÃ³n {i+1}: {sentences[i]}\")\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "id": "DDZNfpzCXjgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbo7gDxNVhFO"
      },
      "source": [
        "## 6. Pipeline Completo de Preprocesamiento\n",
        "\n",
        "Combinamos todos los pasos anteriores en una funciÃ³n reutilizable y parametrizable segÃºn la tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eak4l78DVhFO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re, emoji, nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def pipeline_preprocesamiento(\n",
        "    texto,\n",
        "    normalizar_urls=True,\n",
        "    normalizar_users=True,\n",
        "    normalizar_hashtags=False,\n",
        "    tratar_emojis='conservar',   # 'conservar' | 'eliminar' | 'texto'\n",
        "    lowercase=True,\n",
        "    reduce_elongaciones=True,\n",
        "    eliminar_stopwords=False,\n",
        "    stopwords_extra=None,\n",
        "):\n",
        "    if normalizar_urls:\n",
        "        texto = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', texto)\n",
        "    if normalizar_users:\n",
        "        texto = re.sub(r'@\\w+', '[USER]', texto)\n",
        "    if normalizar_hashtags:\n",
        "        texto = re.sub(r'#(\\w+)', r'\\1', texto)\n",
        "    if tratar_emojis == 'eliminar':\n",
        "        texto = emoji.replace_emoji(texto, replace='')\n",
        "    elif tratar_emojis == 'texto':\n",
        "        texto = emoji.demojize(texto, language='es', delimiters=(' [', '] '))\n",
        "    if reduce_elongaciones:\n",
        "        texto = re.sub(r'(.)\\1{2,}', r'\\1\\1', texto)\n",
        "    tknzr = TweetTokenizer(preserve_case=(not lowercase), reduce_len=reduce_elongaciones)\n",
        "    tokens = tknzr.tokenize(texto)\n",
        "    if eliminar_stopwords:\n",
        "        stop = set(stopwords.words('spanish'))\n",
        "        if stopwords_extra:\n",
        "            stop.update(stopwords_extra)\n",
        "        tokens = [t for t in tokens if t not in stop]\n",
        "    return tokens\n",
        "\n",
        "# â”€â”€ Prueba con distintas configuraciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tweet_test = \"IncreÃ­ble!! ğŸ˜ğŸ˜ @usuario dice que https://t.co/abc es lo mejorrr #PLN\"\n",
        "\n",
        "configs = {\n",
        "    \"MÃ­nimo\"       : dict(eliminar_stopwords=False, tratar_emojis='conservar'),\n",
        "    \"ClasificaciÃ³n\": dict(eliminar_stopwords=True,  tratar_emojis='eliminar',  normalizar_hashtags=True),\n",
        "    \"Sentimiento\"  : dict(eliminar_stopwords=False, tratar_emojis='texto',     normalizar_hashtags=True),\n",
        "}\n",
        "print(f\"Tweet: {tweet_test}\\n\")\n",
        "for nombre, kwargs in configs.items():\n",
        "    tokens = pipeline_preprocesamiento(tweet_test, **kwargs)\n",
        "    print(f\"  [{nombre}]\\n    {tokens}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hppnXSBNVhFO"
      },
      "source": [
        "---\n",
        "# EJERCICIOS\n",
        "\n",
        "Para estos ejercicios, utilizaremos un conjunto de tweets en espaÃ±ol (`cost.csv`, disponible en el material complementario). El objetivo es comparar cÃ³mo afectan las distintas tÃ©cnicas de tokenizaciÃ³n y limpieza a las estadÃ­sticas del corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcI1GIXBVhFO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#A partir de aquÃ­, a trabajar..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNDPAXkUVhFO"
      },
      "source": [
        "#### Ejercicio 1: Limpieza y AnonimizaciÃ³n\n",
        "\n",
        "Crea una funciÃ³n llamada `preprocesar()` que reciba un tweet y:\n",
        "\n",
        "1. Sustituya todas las URLs por `[URL]`.\n",
        "2. Sustituya todas las menciones por `[USER]`.\n",
        "3. Convierta todo el texto a minÃºsculas.\n",
        "4. **Pregunta:** Â¿CuÃ¡ntos tokens Ãºnicos de `[USER]` y `[URL]` hay en el corpus tras procesarlo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98DNtohhVhFO"
      },
      "source": [
        "#### Ejercicio 2: Comparativa de Vocabulario\n",
        "\n",
        "Aplica tres tipos de tokenizaciÃ³n al corpus limpio (Split, NLTK TweetTokenizer y WordPiece/BPE) y calcula lo siguiente para cada uno:\n",
        "1. Tokens totales\n",
        "2. TamaÃ±o del vocabulario (tokens Ãºnicos)\n",
        "\n",
        "Analiza los resultados obtenidos. Â¿QuÃ© hace que el tamaÃ±o del vocabulario sea mayor o menor en funciÃ³n del tokenizador escogido?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqs8yaXEVhFP"
      },
      "source": [
        "#### Ejercicio 3: Stopwords y Frecuencia\n",
        "\n",
        "Utilizando el tokenizador de NLTK:\n",
        "\n",
        "1. Muestra los 10 tokens mÃ¡s frecuentes **con** stopwords.\n",
        "2. Muestra los 10 tokens mÃ¡s frecuentes **sin** stopwords.\n",
        "3. Muestra los 10 tokens mÃ¡s frecuentes **sin** caracteres no alfanumÃ©ricos.\n",
        "\n",
        "Â¿QuÃ© diferencia observas en la capacidad de estos tokens para describir el \"tema\" o temÃ¡tica de los tweets?"
      ]
    }
  ]
}